---
title: "Lab 09"
author: "Yuhong Hu"
date: "`r Sys.Date()`"
output: 
       github_document:
         html_preview: false
       html_document: default
       pdf_document: default
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(parallel)
# tinytex::install_tinytex()
```


# Problem 2: Before you
1. The following functions can be written to be more efficient without using parallel:

This function generates a `n x k` dataset with all its entries distributed poission with mean `lambda`
```{r}
set.seed(1235)
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  
  for (i in 1:n)
    x <- rbind(x, rpois(k, lambda))
  
  return(x)
}
f1<-fun1()
mean(f1)

fun1alt <- function(n = 100, k = 4, lambda = 4) {
  x<-matrix(rpois(n*k, lambda),ncol=4)
}

# Benchmarking
microbenchmark::microbenchmark(
  fun1(),
  fun1alt()
)
```

something about matrix
```{r}
d <- matrix(1:16, ncol=4)
d[2,1]
d[2]
diag(d)
d[c(1,6,11,16)]
cbind(1:4,1:4)
d[cbind(1:4,1:4)]
```

2. Find the column max (hint: Checkout the function `max.col()`).

```{r}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
M <- matrix(runif(12),ncol=4)
M

x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

fun2(M)

fun2alt <- function(x) {
  idx <- max.col(t(x))
  x[cbind(idx,1:4)]
}

fun2alt(M)

# Benchmarking
microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x)
)
```


# Problem 3: Parallelize everything

We will now turn our attention to non-parametric bootstrapping. Among its many uses, non-parametric bootstrapping allow us to obtain confidence intervals for parameter estimates without relying on parametric assumptions.

The main assumption is that we can approximate many experiments by resampling observations from our original dataset, which reflects the population.

This function implements the non-parametric bootstrap:

```{r}
my_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
 
  # Making the cluster using `ncpus`
  cl <- makePSOCKcluster(4)   
  clusterSetRNGStream(cl, 123) # Equivalent to `set.seed(123)`
  # STEP 2: GOES HERE
  clusterExport(cl, c("stat","dat","idx"),envir = environment())

  
    # STEP 3: THIS FUNCTION NEEDS TO BE REPLACES WITH parLapply
  ans <- parLapply(cl,seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })
  # ans <- parApply(
  # cl     = cl,
  # X      = X,
  # MARGIN = 2,
  # FUN    = function(x) coef(lm(y ~ x))
  # )
  # 
  # Coercing the list into a matrix
  ans <- do.call(rbind, ans)
  
  # STEP 4: GOES HERE
  ans
}
```

1. Use the previous pseudocode, and make it work with parallel. Here is just an example for you to try:
```{r,warning=FALSE}
# Bootstrap of an OLS
my_stat <- function(d) coef(lm(y ~ x, data=d))

# DATA SIM
set.seed(1)
n <- 500; R <- 1e4

x <- cbind(rnorm(n)); y <- x*5 + rnorm(n)

# Checking if we get something similar as lm
ans0 <- confint(lm(y~x))
ans1 <- my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 2L)

# You should get something like this
t(apply(ans1, 2, quantile, c(.025,.975)))
##                   2.5%      97.5%
## (Intercept) -0.1372435 0.05074397
## x            4.8680977 5.04539763
ans0
##                  2.5 %     97.5 %
## (Intercept) -0.1379033 0.04797344
## x            4.8650100 5.04883353


```

2. Check whether your version actually goes faster than the non-parallel version:
```{r,warning=FALSE}
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 1L))
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 2L))
```

